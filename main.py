import json
import re
from typing import List, Optional, Dict, Tuple

import uvicorn
from fastapi import FastAPI, Request, Depends
from pydantic import BaseModel
from contextlib import asynccontextmanager
import pymorphy3


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
morph = pymorphy3.MorphAnalyzer()


def normalize_text(text: str) -> str:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç, –ø—Ä–∏–≤–æ–¥—è –≤—Å–µ —Å–ª–æ–≤–∞ –∫ –Ω–∞—á–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ.
    
    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        
    Returns:
        –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å —Å–ª–æ–≤–∞–º–∏ –≤ –Ω–∞—á–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ
    """
    words = text.split()
    normalized_words = []
    
    for word in words:
        clean_word = re.sub(r'[^\w]', '', word)
        if clean_word:
            parsed = morph.parse(clean_word)[0]
            normalized_words.append(parsed.normal_form)
        else:
            normalized_words.append(word)
    
    return ' '.join(normalized_words)


def generate_word_forms(word: str) -> set:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞ (–ø–∞–¥–µ–∂–∏, —á–∏—Å–ª–∞) —á–µ—Ä–µ–∑ pymorphy3.
    
    Args:
        word: –°–ª–æ–≤–æ –≤ –Ω–∞—á–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ
        
    Returns:
        –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –≤—Å–µ—Ö —Ñ–æ—Ä–º —Å–ª–æ–≤–∞
    """
    parsed = morph.parse(word)[0]
    forms = {word.lower()}
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—Å–µ —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞
    for form in parsed.lexeme:
        forms.add(form.word.lower())
    
    return forms


def create_flexible_pattern(alias: str) -> str:
    """
    –°–æ–∑–¥–∞–µ—Ç –≥–∏–±–∫–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –∞–ª–∏–∞—Å–∞ —Å —É—á–µ—Ç–æ–º —Å–∫–ª–æ–Ω–µ–Ω–∏–π.
    
    Args:
        alias: –ê–ª–∏–∞—Å –∑–∞–∫–æ–Ω–∞ –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ
        
    Returns:
        –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ (—Å—Ç—Ä–æ–∫–∞) —Å —É—á–µ—Ç–æ–º —Å–∫–ª–æ–Ω–µ–Ω–∏–π
    """
    # –†–∞–∑–±–∏–≤–∞–µ–º –∞–ª–∏–∞—Å –Ω–∞ —Ç–æ–∫–µ–Ω—ã (—Å–ª–æ–≤–∞ –∏ –Ω–µ-—Å–ª–æ–≤–∞)
    tokens = re.findall(r'[–∞-—è—ëa-z]+|[^–∞-—è—ëa-z]+', alias, re.IGNORECASE)
    
    pattern_parts = []
    
    for token in tokens:
        if re.match(r'^[–∞-—è—ëa-z]+$', token, re.IGNORECASE):
            # –≠—Ç–æ —Å–ª–æ–≤–æ - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—Å–µ —Ñ–æ—Ä–º—ã
            word_forms = generate_word_forms(token)
            
            # –ï—Å–ª–∏ —Å–ª–æ–≤–æ –¥–ª–∏–Ω–Ω–æ–µ (>3 —Å–∏–º–≤–æ–ª–æ–≤) –∏ –∏–º–µ–µ—Ç —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º—ã, —Å–æ–∑–¥–∞–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã
            if len(word_forms) > 1 and len(token) > 3:
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ–æ—Ä–º—ã –ø–æ –¥–ª–∏–Ω–µ (–æ—Ç –¥–ª–∏–Ω–Ω—ã—Ö –∫ –∫–æ—Ä–æ—Ç–∫–∏–º) –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –º–∞—Ç—á–∏–Ω–≥–∞
                sorted_forms = sorted(word_forms, key=len, reverse=True)
                escaped_forms = [re.escape(form) for form in sorted_forms]
                pattern_parts.append(f"(?:{'|'.join(escaped_forms)})")
            else:
                # –ö–æ—Ä–æ—Ç–∫–æ–µ —Å–ª–æ–≤–æ –∏–ª–∏ –Ω–µ—Ç —Ñ–æ—Ä–º - –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                pattern_parts.append(re.escape(token))
        else:
            # –≠—Ç–æ –Ω–µ —Å–ª–æ–≤–æ (–ø—Ä–æ–±–µ–ª—ã, –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è) - –¥–µ–ª–∞–µ–º –≥–∏–±–∫–∏–º
            # –ü—Ä–æ–±–µ–ª—ã –º–æ–≥—É—Ç –±—ã—Ç—å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏ –∏–ª–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏
            if token.strip() == '':
                pattern_parts.append(r'\s+')
            else:
                pattern_parts.append(re.escape(token))
    
    pattern = ''.join(pattern_parts)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã —Å–ª–æ–≤ –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if alias and re.match(r'[\w–∞-—è—ë–ê-–Ø–Å]', alias[0]):
        pattern = r'\b' + pattern
    if alias and re.match(r'[\w–∞-—è—ë–ê-–Ø–Å]', alias[-1]):
        pattern = pattern + r'\b'
    
    return pattern


def load_law_aliases_with_morphology():
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç law_aliases.json –∏ —Å–æ–∑–¥–∞–µ—Ç –∏–Ω–¥–µ–∫—Å —Å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º–∏ —Ñ–æ—Ä–º–∞–º–∏.
    –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ø—Ä–µ–¥–∫–æ–º–ø–∏–ª–∏—Ä—É–µ—Ç –≤—Å–µ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –æ–¥–∏–Ω —Ä–∞–∑.
    –£—á–∏—Ç—ã–≤–∞–µ—Ç —Å–∫–ª–æ–Ω–µ–Ω–∏—è —Å–ª–æ–≤ –≤ –∞–ª–∏–∞—Å–∞—Ö.
    
    Returns:
        Tuple[Dict, List]: (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π_–∞–ª–∏–∞—Å -> [(–æ—Ä–∏–≥–∏–Ω–∞–ª, law_id)], –≤—Å–µ –∞–ª–∏–∞—Å—ã –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ)
    """
    with open('law_aliases.json', 'r', encoding='utf-8') as f:
        law_aliases = json.load(f)
    
    normalized_index = {}
    all_aliases = []
    
    for law_id, aliases in law_aliases.items():
        for alias in aliases:
            alias_lower = alias.lower()
            normalized = normalize_text(alias_lower)
            
            if normalized not in normalized_index:
                normalized_index[normalized] = []
            normalized_index[normalized].append((alias_lower, law_id))
            
            # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: —Å–æ–∑–¥–∞–µ–º –≥–∏–±–∫–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω —Å —É—á–µ—Ç–æ–º —Å–∫–ª–æ–Ω–µ–Ω–∏–π
            flexible_pattern_str = create_flexible_pattern(alias_lower)
            compiled_pattern = re.compile(flexible_pattern_str, re.IGNORECASE)
            
            # –¢–∞–∫–∂–µ —Å–æ–∑–¥–∞–µ–º —Ç–æ—á–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –º–∞—Ç—á–∏–Ω–≥–∞ (–±–µ–∑ —Å–∫–ª–æ–Ω–µ–Ω–∏–π)
            escaped_alias = re.escape(alias_lower)
            if re.match(r'[\w]', alias_lower):
                exact_pattern_str = r'\b' + escaped_alias
            else:
                exact_pattern_str = escaped_alias
            if re.search(r'[\w]$', alias_lower):
                exact_pattern_str = exact_pattern_str + r'\b'
            
            exact_compiled_pattern = re.compile(exact_pattern_str, re.IGNORECASE)
            
            all_aliases.append({
                'original': alias_lower,
                'normalized': normalized,
                'law_id': law_id,
                'length': len(alias_lower),
                'word_count': len(alias_lower.split()),
                'compiled_pattern': compiled_pattern,  # –ì–∏–±–∫–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω —Å —É—á–µ—Ç–æ–º —Å–∫–ª–æ–Ω–µ–Ω–∏–π
                'exact_pattern': exact_compiled_pattern  # –¢–æ—á–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
            })
    
    all_aliases.sort(key=lambda x: x['length'], reverse=True)
    
    return normalized_index, all_aliases


def find_law_in_text(text: str, normalized_index: Dict, all_aliases: List[Dict]) -> Optional[str]:
    """
    –ò—â–µ—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –∫–æ–¥–µ–∫—Å–∞ –≤ —Ç–µ–∫—Å—Ç–µ —Å —É—á–µ—Ç–æ–º —Å–∫–ª–æ–Ω–µ–Ω–∏–π.
    
    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞
        normalized_index: –ò–Ω–¥–µ–∫—Å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–æ—Ä–º
        all_aliases: –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∞–ª–∏–∞—Å–æ–≤
        
    Returns:
        law_id –∏–ª–∏ None
    """
    text_lower = text.lower()
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, ‚Ññ474, ‚Ññ201-—Ä–ø)
    numbers_in_text = re.findall(r'‚Ññ\s*(\d+(?:[-./]\S*)?)', text)
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ (–£–∫–∞–∑, –†–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–µ, –ü–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏ —Ç.–¥.)
    doc_type_in_text = None
    doc_type_patterns = {
        '—É–∫–∞–∑': r'—É–∫–∞–∑[–∞–µ—É—ã–æ–∏]?\b',
        '—Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω': r'—Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω[–∏–µ—é—è]+\b',
        '–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω': r'–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω[–∏–µ—é—è]+\b',
        '–ø—Ä–∏–∫–∞–∑': r'–ø—Ä–∏–∫–∞–∑[–∞–µ—É—ã–æ–∏]?\b',
        '–∑–∞–∫–æ–Ω': r'–∑–∞–∫–æ–Ω[–∞–µ—É—ã–æ–∏]?\b'
    }
    
    for base_type, pattern in doc_type_patterns.items():
        if re.search(pattern, text_lower):
            doc_type_in_text = base_type
            break
    
    # –ï—Å–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –Ω–æ–º–µ—Ä–∞, —Å–æ–±–∏—Ä–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    best_match_without_number = None
    best_match_score = 0
    
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø—Ä—è–º–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–±—ã—Å—Ç—Ä–µ–µ –∏ —Ç–æ—á–Ω–µ–µ)
    # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
    for alias_data in all_aliases:
        alias_lower = alias_data['original']
        
        # –ï—Å–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å –Ω–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞, –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å –∞–ª–∏–∞—Å–æ–º
        if numbers_in_text:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä–∞ –∏–∑ –∞–ª–∏–∞—Å–∞
            alias_numbers = re.findall(r'‚Ññ\s*(\d+(?:[-./]\S*)?)', alias_lower)
            
            # –ï—Å–ª–∏ —É –∞–ª–∏–∞—Å–∞ –µ—Å—Ç—å –Ω–æ–º–µ—Ä, –æ–Ω –¥–æ–ª–∂–µ–Ω —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å –Ω–æ–º–µ—Ä–æ–º –≤ —Ç–µ–∫—Å—Ç–µ
            if alias_numbers:
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–æ–º–µ—Ä–∞ (—É–±–∏—Ä–∞–µ–º —Ç–æ—á–∫–∏ –∏ –¥—Ä—É–≥–∏–µ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –≤ –∫–æ–Ω—Ü–µ)
                normalized_text_numbers = [num.rstrip('.,;:!?') for num in numbers_in_text]
                normalized_alias_numbers = [num.rstrip('.,;:!?') for num in alias_numbers]
                has_matching_number = any(alias_num in normalized_text_numbers for alias_num in normalized_alias_numbers)
                if not has_matching_number:
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç –∞–ª–∏–∞—Å, —Ç–∞–∫ –∫–∞–∫ –Ω–æ–º–µ—Ä –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç
                    continue
                else:
                    # –ï—Å–ª–∏ –Ω–æ–º–µ—Ä —Å–æ–≤–ø–∞–¥–∞–µ—Ç –∏ —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å–æ–≤–ø–∞–¥–∞–µ—Ç,
                    # –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (–±–æ–ª–µ–µ –≥–∏–±–∫–æ, —Å —É—á–µ—Ç–æ–º —Å–∫–ª–æ–Ω–µ–Ω–∏–π)
                    if doc_type_in_text and doc_type_in_text in alias_lower:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∞–ª–∏–∞—Å–∞ (—Å–ª–æ–≤–∞ –¥–ª–∏–Ω–Ω–µ–µ 3 —Å–∏–º–≤–æ–ª–æ–≤, –±—É–∫–≤–µ–Ω–Ω—ã–µ)
                        alias_words = re.findall(r'\b[–∞-—è—ëa-z]{4,}\b', alias_lower)
                        if alias_words:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ —Ç–µ–∫—Å—Ç–µ
                            matching_count = sum(1 for word in alias_words if word in text_lower)
                            # –ï—Å–ª–∏ —Ö–æ—Ç—è –±—ã 60% –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Å–æ–≤–ø–∞–¥–∞—é—Ç, —Å—á–∏—Ç–∞–µ–º —ç—Ç–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ–º
                            if matching_count >= len(alias_words) * 0.6:
                                return alias_data['law_id']
        
        # –ï—Å–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞, –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å –∞–ª–∏–∞—Å–æ–º
        if doc_type_in_text:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∞–ª–∏–∞—Å —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ—Ç –∂–µ —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
            if doc_type_in_text not in alias_lower:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç –∞–ª–∏–∞—Å, —Ç–∞–∫ –∫–∞–∫ —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç
                continue
            
            # –ï—Å–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ –ù–ï–¢ –Ω–æ–º–µ—Ä–∞, —Å–æ–±–∏—Ä–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (–¥–ª—è —É—á–µ—Ç–∞ —Å–∫–ª–æ–Ω–µ–Ω–∏–π)
            if not numbers_in_text:
                alias_words = re.findall(r'\b[–∞-—è—ëa-z]{4,}\b', alias_lower)
                if alias_words:
                    matching_count = sum(1 for word in alias_words if word in text_lower)
                    match_ratio = matching_count / len(alias_words)
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (—Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –ø—Ä–æ—Ü–µ–Ω—Ç–æ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è)
                    if match_ratio > best_match_score and match_ratio >= 0.7:
                        best_match_score = match_ratio
                        best_match_without_number = alias_data['law_id']
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–±—ã—Å—Ç—Ä–µ–µ)
        if alias_data['exact_pattern'].search(text_lower):
            return alias_data['law_id']
        
        # –ï—Å–ª–∏ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –Ω–µ—Ç, –ø—Ä–æ–±—É–µ–º –≥–∏–±–∫–æ–µ (—Å —É—á–µ—Ç–æ–º —Å–∫–ª–æ–Ω–µ–Ω–∏–π)
        if alias_data['compiled_pattern'].search(text_lower):
            return alias_data['law_id']
    
    # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–π –∞–ª–∏–∞—Å –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (–¥–ª—è —Å–ª—É—á–∞—è –±–µ–∑ –Ω–æ–º–µ—Ä–∞ –≤ —Ç–µ–∫—Å—Ç–µ)
    if best_match_without_number:
        return best_match_without_number
    
    # –ï—Å–ª–∏ –ø—Ä—è–º–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –Ω–µ—Ç, –ø—Ä–æ–±—É–µ–º —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π
    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Ñ—Ä–∞–∑—ã (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤)
    word_sequences = re.finditer(r'[–∞-—è—ë–ê-–Ø–Å\w\s]+', text_lower)
    
    for seq_match in word_sequences:
        sequence = seq_match.group()
        words = sequence.split()
        
        max_window = min(10, len(words))
        
        for window_size in range(max_window, 0, -1):
            for i in range(len(words) - window_size + 1):
                window = words[i:i + window_size]
                phrase = ' '.join(window)
                
                normalized_phrase = normalize_text(phrase)
                
                if normalized_phrase in normalized_index:
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –∞–ª–∏–∞—Å–æ–≤
                    matches = normalized_index[normalized_phrase]
                    
                    for original_alias, law_id in matches:
                        # –ï—Å–ª–∏ –∞–ª–∏–∞—Å –∫–æ—Ä–æ—Ç–∫–∏–π (–∫–∞–∫ "–ù–ö", "–ì–ö"), —Ç—Ä–µ–±—É–µ–º —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                        if len(original_alias) <= 3:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ—Ä–∞–∑–∞ –≤ —Ç–µ–∫—Å—Ç–µ –≤—ã–≥–ª—è–¥–∏—Ç –∫–∞–∫ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞
                            if re.search(r'\b' + re.escape(phrase.upper()) + r'\b', text.upper()):
                                return law_id
                        else:
                            return law_id
    
    return None


def parse_legal_reference_v2(text: str, normalized_index: Dict, all_aliases: List[Dict]) -> List[Dict[str, Optional[str]]]:
    """
    –ü–∞—Ä—Å–∏—Ç —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π —Ç–µ–∫—Å—Ç –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Å—Ç–∞—Ç–µ–π, –ø—É–Ω–∫—Ç–æ–≤ –∏ –ø–æ–¥–ø—É–Ω–∫—Ç–æ–≤.
    –í–µ—Ä—Å–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å–∫–ª–æ–Ω–µ–Ω–∏–π —á–µ—Ä–µ–∑ pymorphy3 –∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫.
    
    Args:
        text: –¢–µ–∫—Å—Ç —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏—è–º–∏ —Å—Ç–∞—Ç–µ–π, –ø—É–Ω–∫—Ç–æ–≤ –∏/–∏–ª–∏ –ø–æ–¥–ø—É–Ω–∫—Ç–æ–≤
        normalized_index: –ò–Ω–¥–µ–∫—Å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–æ—Ä–º
        all_aliases: –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∞–ª–∏–∞—Å–æ–≤
        
    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –ø–æ–ª—è–º–∏ law_id, article, point_article, subpoint_article
    """
    results = []
    
    text_lower = text.lower().strip()
    text_stripped = text.strip()
    
    # –ü–æ–∏—Å–∫ law_id —Å —É—á–µ—Ç–æ–º —Å–∫–ª–æ–Ω–µ–Ω–∏–π
    law_id = find_law_in_text(text_lower, normalized_index, all_aliases)
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Ç–∞—Ç–µ–π —Å –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è–º–∏
    article_list_patterns = [
        r'—Å—Ç–∞—Ç(?:—å(?:—è–º–∏|—è—Ö|—è–º|—ë–π|–µ–π|[—è–∏–µ—é])|–µ–π)\s+((?:\d+(?:[.-]\d+)*(?:\s*,\s*|\s+–∏\s+|\s+–∏–ª–∏\s+)?)+)',
        r'—Å—Ç\.?\s*((?:\d+(?:[.-]\d+)*(?:\s*,\s*|\s+–∏\s+|\s+–∏–ª–∏\s+)?)+)',
    ]
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø—É–Ω–∫—Ç–æ–≤ —Å –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è–º–∏
    point_list_patterns = [
        r'(?<!–ø–æ–¥)–ø—É–Ω–∫—Ç[–∞–µ—É—ã–æ–∏]?\s+((?:(?:\d+[–∞-—è–ê-–Ø]?|[–∞-—è–ê-–Ø])(?:\s*,\s*(?:(?:–∏|–∏–ª–∏)\s+)?|\s+(?:–∏|–∏–ª–∏)\s+)?)+)',
        r'(?<![–∞-—è–ê-–Ø])–ø\.?\s+((?:(?:\d+[–∞-—è–ê-–Ø]?|[–∞-—è–ê-–Ø])(?:\s*,\s*(?:(?:–∏|–∏–ª–∏)\s+)?|\s+(?:–∏|–∏–ª–∏)\s+)?)+)',
    ]
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ–¥–ø—É–Ω–∫—Ç–æ–≤ —Å –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è–º–∏
    subpoint_list_patterns = [
        r'–ø–æ–¥–ø—É–Ω–∫—Ç[–∞–µ—É—ã–æ–∏]?\s+((?:(?:[–∞-—è–ê-–Ø](?!\.)\d*|\d+)(?:\s*,\s*(?:(?:–∏|–∏–ª–∏)\s+)?|\s+(?:–∏|–∏–ª–∏)\s+)?)+)',
        r'–ø–æ–¥–ø\.?\s+((?:(?:[–∞-—è–ê-–Ø](?!\.)\d*|\d+)(?:\s*,\s*(?:(?:–∏|–∏–ª–∏)\s+)?|\s+(?:–∏|–∏–ª–∏)\s+)?)+)',
        r'–ø–ø\.?\s+((?:(?:[–∞-—è–ê-–Ø](?!\.)\d*|\d+)(?:\s*,\s*(?:(?:–∏|–∏–ª–∏)\s+)?|\s+(?:–∏|–∏–ª–∏)\s+)?)+)',
    ]
    
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞–∑–±–æ—Ä–∞ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è
    def parse_enumeration(enum_str: str, pattern: str) -> List[str]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Å—Ç—Ä–æ–∫—É –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è —Ä–µ–≥–∏—Å—Ç—Ä"""
        # –ó–∞–º–µ–Ω—è–µ–º —Å–æ—é–∑—ã –Ω–∞ –∑–∞–ø—è—Ç—ã–µ –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏—è (case-insensitive)
        enum_str = re.sub(r'\s+–∏\s+', ',', enum_str, flags=re.IGNORECASE)
        enum_str = re.sub(r'\s+–∏–ª–∏\s+', ',', enum_str, flags=re.IGNORECASE)
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∑–∞–ø—è—Ç—ã–º
        items = [item.strip() for item in enum_str.split(',')]
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ –∏ –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—É
        return [item for item in items if item and re.match(pattern, item, re.IGNORECASE)]
    
    # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π
    articles_found = []
    for pattern in article_list_patterns:
        matches = re.finditer(pattern, text_lower)
        for match in matches:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞, —Å–æ—Ö—Ä–∞–Ω—è—è —Ä–µ–≥–∏—Å—Ç—Ä
            enum_str = text_stripped[match.start(1):match.end(1)]
            articles = parse_enumeration(enum_str, r'^\d+(?:[.-]\d+)*$')
            articles_found.extend(articles)
    
    # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö –ø—É–Ω–∫—Ç–æ–≤
    points_found = []
    for pattern in point_list_patterns:
        matches = re.finditer(pattern, text_lower)
        for match in matches:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞, —Å–æ—Ö—Ä–∞–Ω—è—è —Ä–µ–≥–∏—Å—Ç—Ä
            enum_str = text_stripped[match.start(1):match.end(1)]
            points = parse_enumeration(enum_str, r'^(?:\d+[–∞-—è–ê-–Ø]?|[–∞-—è–ê-–Ø])$')
            # –§–∏–ª—å—Ç—Ä—É–µ–º "–ø" - —ç—Ç–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –¥–ª—è —Å–ª–æ–≤–∞ "–ø—É–Ω–∫—Ç", –∞ –Ω–µ –Ω–æ–º–µ—Ä –ø—É–Ω–∫—Ç–∞
            points = [p for p in points if p.lower() != '–ø']
            points_found.extend(points)
    
    # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö –ø–æ–¥–ø—É–Ω–∫—Ç–æ–≤
    subpoints_found = []
    for pattern in subpoint_list_patterns:
        matches = re.finditer(pattern, text_lower)
        for match in matches:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞, —Å–æ—Ö—Ä–∞–Ω—è—è —Ä–µ–≥–∏—Å—Ç—Ä
            enum_str = text_stripped[match.start(1):match.end(1)]
            # –ü–æ–¥–ø—É–Ω–∫—Ç—ã: –±—É–∫–≤–∞ (—è), –±—É–∫–≤–∞+—Ü–∏—Ñ—Ä—ã (—è1), –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ —Ü–∏—Ñ—Ä—ã (26)
            subpoints = parse_enumeration(enum_str, r'^(?:[–∞-—è–ê-–Ø]\d*|\d+)$')
            # –§–∏–ª—å—Ç—Ä—É–µ–º "–ø" –∏ "–ø–æ–¥–ø" - —ç—Ç–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –¥–ª—è —Å–ª–æ–≤–∞ "–ø—É–Ω–∫—Ç/–ø–æ–¥–ø—É–Ω–∫—Ç", –∞ –Ω–µ –Ω–æ–º–µ—Ä–∞
            subpoints = [s for s in subpoints if s.lower() not in ['–ø', '–ø–æ–¥–ø', '–ø–ø']]
            subpoints_found.extend(subpoints)
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º law_id –∏–∑ —Å—Ç—Ä–æ–∫–∏ –≤ —á–∏—Å–ª–æ
    law_id_int = int(law_id) if law_id is not None else None
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
    if subpoints_found and len(subpoints_found) > 1:
        # –ï—Å—Ç—å –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ–¥–ø—É–Ω–∫—Ç–æ–≤ - —Å–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ
        for subpoint in subpoints_found:
            result = {
                "law_id": law_id_int,
                "article": articles_found[0] if articles_found else None,
                "point_article": points_found[0] if points_found else None,
                "subpoint_article": subpoint
            }
            results.append(result)
    elif points_found and len(points_found) > 1:
        # –ï—Å—Ç—å –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç–æ–≤ - —Å–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ
        for point in points_found:
            result = {
                "law_id": law_id_int,
                "article": articles_found[0] if articles_found else None,
                "point_article": point,
                "subpoint_article": subpoints_found[0] if subpoints_found else None
            }
            results.append(result)
    elif articles_found and len(articles_found) > 1:
        # –ï—Å—Ç—å –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–µ–π - —Å–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –¥–ª—è –∫–∞–∂–¥–æ–π
        for article in articles_found:
            result = {
                "law_id": law_id_int,
                "article": article,
                "point_article": points_found[0] if points_found else None,
                "subpoint_article": subpoints_found[0] if subpoints_found else None
            }
            results.append(result)
    elif articles_found or points_found or subpoints_found:
        # –ï–¥–∏–Ω–∏—á–Ω–æ–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ
        result = {
            "law_id": law_id_int,
            "article": articles_found[0] if articles_found else None,
            "point_article": points_found[0] if points_found else None,
            "subpoint_article": subpoints_found[0] if subpoints_found else None
        }
        results.append(result)
    else:
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—É—é –∑–∞–ø–∏—Å—å
        results.append({
            "law_id": law_id_int,
            "article": None,
            "point_article": None,
            "subpoint_article": None
        })
    
    return results


def parse_legal_reference_multi_law(text: str, normalized_index: Dict, all_aliases: List[Dict]) -> List[Dict[str, Optional[str]]]:
    """
    –ü–∞—Ä—Å–∏—Ç —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π —Ç–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑–Ω—ã—Ö –∑–∞–∫–æ–Ω–æ–≤.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è —Ç–æ—á–Ω–æ–π –ø—Ä–∏–≤—è–∑–∫–∏ —Å—Ç–∞—Ç–µ–π –∫ –∑–∞–∫–æ–Ω–∞–º.
    
    Args:
        text: –¢–µ–∫—Å—Ç —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏—è–º–∏ —Å—Ç–∞—Ç–µ–π, –ø—É–Ω–∫—Ç–æ–≤ –∏/–∏–ª–∏ –ø–æ–¥–ø—É–Ω–∫—Ç–æ–≤ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∑–∞–∫–æ–Ω–æ–≤
        normalized_index: –ò–Ω–¥–µ–∫—Å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–æ—Ä–º
        all_aliases: –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∞–ª–∏–∞—Å–æ–≤
        
    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –ø–æ–ª—è–º–∏ law_id, article, point_article, subpoint_article
    """
    results = []
    text_lower = text.lower()
    
    # –®–∞–≥ 1: –ù–∞–π—Ç–∏ –≤—Å–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∑–∞–∫–æ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ
    law_mentions = []
    for alias_data in all_aliases:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        for match in alias_data['exact_pattern'].finditer(text_lower):
            law_mentions.append({
                'law_id': alias_data['law_id'],
                'start': match.start(),
                'end': match.end(),
                'text': match.group()
            })
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è, –ø—Ä–æ–±—É–µ–º –≥–∏–±–∫–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω
        if not law_mentions or alias_data['law_id'] not in [m['law_id'] for m in law_mentions]:
            for match in alias_data['compiled_pattern'].finditer(text_lower):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ –¥—É–±–ª–∏–∫–∞—Ç
                if not any(m['start'] <= match.start() < m['end'] or m['start'] < match.end() <= m['end'] 
                          for m in law_mentions):
                    law_mentions.append({
                        'law_id': alias_data['law_id'],
                        'start': match.start(),
                        'end': match.end(),
                        'text': match.group()
                    })
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ–∑–∏—Ü–∏–∏
    seen = set()
    unique_mentions = []
    for mention in law_mentions:
        key = (mention['law_id'], mention['start'], mention['end'])
        if key not in seen:
            seen.add(key)
            unique_mentions.append(mention)
    
    law_mentions = sorted(unique_mentions, key=lambda x: x['start'])
    
    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –Ω–∏ –æ–¥–Ω–æ–≥–æ –∑–∞–∫–æ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—É—é —Ñ—É–Ω–∫—Ü–∏—é
    if not law_mentions:
        return parse_legal_reference_v2(text, normalized_index, all_aliases)
    
    # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∑–∞–∫–æ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—É—é —Ñ—É–Ω–∫—Ü–∏—é
    if len(law_mentions) == 1:
        return parse_legal_reference_v2(text, normalized_index, all_aliases)
    
    # –®–∞–≥ 2: –ù–∞–π—Ç–∏ –≤—Å–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Å—Ç–∞—Ç–µ–π, –ø—É–Ω–∫—Ç–æ–≤, –ø–æ–¥–ø—É–Ω–∫—Ç–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ
    legal_references = []
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Å—Ç–∞—Ç–µ–π —Å –ø—É–Ω–∫—Ç–∞–º–∏ –∏ –ø–æ–¥–ø—É–Ω–∫—Ç–∞–º–∏
    combined_patterns = [
        # –ø–ø. X –ø. Y —Å—Ç. Z
        r'(?:–ø–ø\.?\s+([–∞-—è–ê-–Ø]\d*|\d+)\s+)?(?:–ø\.?\s+(\d+[–∞-—è–ê-–Ø]?|[–∞-—è–ê-–Ø])\s+)?—Å—Ç\.?\s+(\d+(?:[.-]\d+)*)',
        # –ø–æ–¥–ø. X –ø. Y —Å—Ç. Z
        r'(?:–ø–æ–¥–ø\.?\s+([–∞-—è–ê-–Ø]\d*|\d+)\s+)?(?:–ø\.?\s+(\d+[–∞-—è–ê-–Ø]?|[–∞-—è–ê-–Ø])\s+)?—Å—Ç–∞—Ç(?:—å[—è–∏–µ—é]|–µ–π)\s+(\d+(?:[.-]\d+)*)',
    ]
    
    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å –ø–æ–ª–Ω—ã–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏ (—Å–æ —Å—Ç–∞—Ç—å—è–º–∏)
    found_positions = set()
    for pattern in combined_patterns:
        for match in re.finditer(pattern, text_lower):
            subpoint = match.group(1) if match.lastindex >= 1 and match.group(1) else None
            point = match.group(2) if match.lastindex >= 2 and match.group(2) else None
            article = match.group(3) if match.lastindex >= 3 else None
            
            if article:
                legal_references.append({
                    'position': match.start(),
                    'article': article,
                    'point': point,
                    'subpoint': subpoint,
                    'law_id': None  # –ë—É–¥–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω –ø–æ–∑–∂–µ
                })
                # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –ø–æ–∑–∏—Ü–∏–∏, —á—Ç–æ–±—ã –Ω–µ –¥—É–±–ª–∏—Ä–æ–≤–∞—Ç—å
                found_positions.add(match.start())
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø—É–Ω–∫—Ç–æ–≤ –ë–ï–ó —Å—Ç–∞—Ç–µ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–ø. 10 –ê–ü–ö")
    point_only_patterns = [
        r'(?<!–ø–æ–¥)–ø—É–Ω–∫—Ç[–∞–µ—É—ã–æ–∏]?\s+(\d+[–∞-—è–ê-–Ø]?|[–∞-—è–ê-–Ø])(?!\s+—Å—Ç)',
        r'(?<![–∞-—è–ê-–Ø])–ø\.?\s+(\d+[–∞-—è–ê-–Ø]?|[–∞-—è–ê-–Ø])(?!\s+—Å—Ç)',
    ]
    
    for pattern in point_only_patterns:
        for match in re.finditer(pattern, text_lower):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–∞ –ø–æ–∑–∏—Ü–∏—è –µ—â–µ –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞
            if match.start() not in found_positions:
                point = match.group(1)
                if point and point.lower() not in ['–ø', '–ø—É–Ω–∫—Ç']:
                    legal_references.append({
                        'position': match.start(),
                        'article': None,
                        'point': point,
                        'subpoint': None,
                        'law_id': None  # –ë—É–¥–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω –ø–æ–∑–∂–µ
                    })
                    found_positions.add(match.start())
    
    # –®–∞–≥ 3: –ü—Ä–∏–≤—è–∑–∞—Ç—å –∫–∞–∂–¥—É—é —Å—Ç–∞—Ç—å—é –∫ –±–ª–∏–∂–∞–π—à–µ–º—É –∑–∞–∫–æ–Ω—É
    for ref in legal_references:
        ref_pos = ref['position']
        
        # –ò—â–µ–º –±–ª–∏–∂–∞–π—à–∏–π –∑–∞–∫–æ–Ω (—Å–ª–µ–≤–∞ –∏–ª–∏ —Å–ø—Ä–∞–≤–∞)
        min_distance = float('inf')
        closest_law = None
        
        for mention in law_mentions:
            # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∑–∞–∫–æ–Ω–∞
            if ref_pos < mention['start']:
                # –°—Ç–∞—Ç—å—è –Ω–∞—Ö–æ–¥–∏—Ç—Å—è —Å–ª–µ–≤–∞ –æ—Ç –∑–∞–∫–æ–Ω–∞
                distance = mention['start'] - ref_pos
            else:
                # –°—Ç–∞—Ç—å—è –Ω–∞—Ö–æ–¥–∏—Ç—Å—è —Å–ø—Ä–∞–≤–∞ –æ—Ç –∑–∞–∫–æ–Ω–∞
                distance = ref_pos - mention['end']
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
            max_distance_before = 150  # –°—Ç–∞—Ç—å—è –º–æ–∂–µ—Ç –±—ã—Ç—å –º–∞–∫—Å–∏–º—É–º –Ω–∞ 150 —Å–∏–º–≤–æ–ª–æ–≤ –ª–µ–≤–µ–µ –∑–∞–∫–æ–Ω–∞
            max_distance_after = 50    # –°—Ç–∞—Ç—å—è –º–æ–∂–µ—Ç –±—ã—Ç—å –º–∞–∫—Å–∏–º—É–º –Ω–∞ 50 —Å–∏–º–≤–æ–ª–æ–≤ –ø—Ä–∞–≤–µ–µ –∑–∞–∫–æ–Ω–∞
            
            if ref_pos < mention['start']:
                if distance > max_distance_before:
                    continue
            else:
                if distance > max_distance_after:
                    continue
            
            if distance < min_distance:
                min_distance = distance
                closest_law = mention
        
        if closest_law:
            ref['law_id'] = closest_law['law_id']
    
    # –®–∞–≥ 4: –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    for ref in legal_references:
        if ref['law_id'] is not None:
            results.append({
                'law_id': int(ref['law_id']),
                'article': ref['article'],
                'point_article': ref['point'],
                'subpoint_article': ref['subpoint']
            })
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    unique_results = []
    seen_results = set()
    for result in results:
        key = (result['law_id'], result['article'], result['point_article'], result['subpoint_article'])
        if key not in seen_results:
            seen_results.add(key)
            unique_results.append(result)
    
    return unique_results if unique_results else results


class LawLink(BaseModel):
    law_id: Optional[int] = None
    article: Optional[str] = None
    point_article: Optional[str] = None
    subpoint_article: Optional[str] = None


class LinksResponse(BaseModel):
    links: List[LawLink]


class TextRequest(BaseModel):
    text: str


@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    print("üöÄ –°–µ—Ä–≤–∏—Å –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è...")
    print("üìö –ó–∞–≥—Ä—É–∑–∫–∞ law_aliases.json –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞...")
    
    normalized_index, all_aliases = load_law_aliases_with_morphology()
    
    app.state.normalized_index = normalized_index
    app.state.all_aliases = all_aliases
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_aliases)} –∞–ª–∏–∞—Å–æ–≤")
    print("üéâ –°–µ—Ä–≤–∏—Å –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")
    
    yield
    
    # Shutdown
    print("üõë –°–µ—Ä–≤–∏—Å –∑–∞–≤–µ—Ä—à–∞–µ—Ç—Å—è...")
    del normalized_index
    del all_aliases


def get_law_data(request: Request) -> Tuple[Dict, List[Dict]]:
    """–ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ–± –∞–ª–∏–∞—Å–∞—Ö –∑–∞–∫–æ–Ω–æ–≤ –∏–∑ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    return request.app.state.normalized_index, request.app.state.all_aliases


app = FastAPI(
    title="Law Links Service",
    description="C–µ—Ä–≤–∏—Å –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Å—Å—ã–ª–æ–∫ –∏–∑ —Ç–µ–∫—Å—Ç–∞",
    version="1.0.0",
    lifespan=lifespan
)


@app.post("/detect")
async def get_law_links(
    data: TextRequest,
    request: Request,
    law_data: Tuple[Dict, List[Dict]] = Depends(get_law_data),
    ) -> LinksResponse:
    """
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Å—Å—ã–ª–æ–∫
    """
    normalized_index, all_aliases = law_data
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º parse_legal_reference_multi_law –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Å—Å—ã–ª–æ–∫
    results = parse_legal_reference_multi_law(data.text, normalized_index, all_aliases)
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –æ–±—ä–µ–∫—Ç—ã LawLink
    links = [
        LawLink(
            law_id=result.get("law_id"),
            article=result.get("article"),
            point_article=result.get("point_article"),
            subpoint_article=result.get("subpoint_article")
        )
        for result in results
    ]
    
    return LinksResponse(links=links)


@app.get("/health")
async def health_check():
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞
    """
    return {"status": "healthy"}



if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8978)
