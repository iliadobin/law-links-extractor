{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "ТЕСТИРОВАНИЕ ВЕРСИИ 2 (с поддержкой склонений через pymorphy3)\n",
            "================================================================================\n",
            "\n",
            "Входной текст: 'В ходе судебного заседания по вопросу наследования было установлено, что права истца на получение имущества защищены законодательством. В частности, это регулируется пп. ж п. 2 ст. 1506 Семейного кодекса РФ, что позволяет оспорить права других наследников. Суд принял решение в пользу истца, основываясь на данной норме законодательства.'\n",
            "Результат: {'law_id': '8', 'article': '1506', 'point_article': '2', 'subpoint_article': 'ж'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import json\n",
        "from typing import Dict, Optional, List, Tuple\n",
        "import pymorphy3\n",
        "\n",
        "# Инициализируем морфологический анализатор\n",
        "morph = pymorphy3.MorphAnalyzer()\n",
        "\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Нормализует текст, приводя все слова к начальной форме.\n",
        "    \n",
        "    Args:\n",
        "        text: Исходный текст\n",
        "        \n",
        "    Returns:\n",
        "        Нормализованный текст с словами в начальной форме\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    normalized_words = []\n",
        "    \n",
        "    for word in words:\n",
        "        clean_word = re.sub(r'[^\\w]', '', word)\n",
        "        if clean_word:\n",
        "            parsed = morph.parse(clean_word)[0]\n",
        "            normalized_words.append(parsed.normal_form)\n",
        "        else:\n",
        "            normalized_words.append(word)\n",
        "    \n",
        "    return ' '.join(normalized_words)\n",
        "\n",
        "\n",
        "def load_law_aliases_with_morphology():\n",
        "    \"\"\"\n",
        "    Загружает law_aliases.json и создает индекс с нормализованными формами.\n",
        "    ОПТИМИЗАЦИЯ: предкомпилирует все регулярные выражения один раз.\n",
        "    \n",
        "    Returns:\n",
        "        Tuple[Dict, List]: (нормализованный_алиас -> [(оригинал, law_id)], все алиасы отсортированные)\n",
        "    \"\"\"\n",
        "    with open('law_aliases.json', 'r', encoding='utf-8') as f:\n",
        "        law_aliases = json.load(f)\n",
        "    \n",
        "    normalized_index = {}\n",
        "    all_aliases = []\n",
        "    \n",
        "    for law_id, aliases in law_aliases.items():\n",
        "        for alias in aliases:\n",
        "            alias_lower = alias.lower()\n",
        "            normalized = normalize_text(alias_lower)\n",
        "            \n",
        "            if normalized not in normalized_index:\n",
        "                normalized_index[normalized] = []\n",
        "            normalized_index[normalized].append((alias_lower, law_id))\n",
        "            \n",
        "            # ОПТИМИЗАЦИЯ: предкомпилируем регулярное выражение один раз\n",
        "            # Используем границы слов только для буквенно-цифровых символов\n",
        "            escaped_alias = re.escape(alias_lower)\n",
        "            \n",
        "            # Добавляем границу слова в начале, только если алиас начинается с буквы/цифры\n",
        "            if re.match(r'[\\w]', alias_lower):\n",
        "                pattern = r'\\b' + escaped_alias\n",
        "            else:\n",
        "                pattern = escaped_alias\n",
        "            \n",
        "            # Добавляем границу слова в конце, только если алиас заканчивается буквой/цифрой\n",
        "            if re.search(r'[\\w]$', alias_lower):\n",
        "                pattern = pattern + r'\\b'\n",
        "            \n",
        "            compiled_pattern = re.compile(pattern)\n",
        "            \n",
        "            all_aliases.append({\n",
        "                'original': alias_lower,\n",
        "                'normalized': normalized,\n",
        "                'law_id': law_id,\n",
        "                'length': len(alias_lower),\n",
        "                'word_count': len(alias_lower.split()),\n",
        "                'compiled_pattern': compiled_pattern  # Сохраняем скомпилированный паттерн\n",
        "            })\n",
        "    \n",
        "    all_aliases.sort(key=lambda x: x['length'], reverse=True)\n",
        "    \n",
        "    return normalized_index, all_aliases\n",
        "\n",
        "\n",
        "def find_law_in_text(text: str, normalized_index: Dict, all_aliases: List[Dict]) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Ищет упоминание кодекса в тексте с учетом склонений.\n",
        "    \n",
        "    Args:\n",
        "        text: Текст для поиска\n",
        "        normalized_index: Индекс нормализованных форм\n",
        "        all_aliases: Список всех алиасов\n",
        "        \n",
        "    Returns:\n",
        "        law_id или None\n",
        "    \"\"\"\n",
        "    text_lower = text.lower()\n",
        "    \n",
        "    # Сначала пробуем прямое совпадение (быстрее и точнее)\n",
        "    # ОПТИМИЗАЦИЯ: используем предкомпилированные регулярные выражения\n",
        "    for alias_data in all_aliases:\n",
        "        # Используем уже скомпилированный паттерн - в разы быстрее!\n",
        "        if alias_data['compiled_pattern'].search(text_lower):\n",
        "            return alias_data['law_id']\n",
        "    \n",
        "    # Если прямого совпадения нет, пробуем с нормализацией\n",
        "    # Разбиваем текст на фразы (последовательности слов)\n",
        "    word_sequences = re.finditer(r'[а-яёА-ЯЁ\\w\\s]+', text_lower)\n",
        "    \n",
        "    for seq_match in word_sequences:\n",
        "        sequence = seq_match.group()\n",
        "        words = sequence.split()\n",
        "        \n",
        "        max_window = min(10, len(words))\n",
        "        \n",
        "        for window_size in range(max_window, 0, -1):\n",
        "            for i in range(len(words) - window_size + 1):\n",
        "                window = words[i:i + window_size]\n",
        "                phrase = ' '.join(window)\n",
        "                \n",
        "                normalized_phrase = normalize_text(phrase)\n",
        "                \n",
        "                if normalized_phrase in normalized_index:\n",
        "                    # Дополнительная проверка для коротких алиасов\n",
        "                    matches = normalized_index[normalized_phrase]\n",
        "                    \n",
        "                    for original_alias, law_id in matches:\n",
        "                        # Если алиас короткий (как \"НК\", \"ГК\"), требуем точного совпадения\n",
        "                        if len(original_alias) <= 3:\n",
        "                            # Проверяем, что фраза в тексте выглядит как аббревиатура\n",
        "                            if re.search(r'\\b' + re.escape(phrase.upper()) + r'\\b', text.upper()):\n",
        "                                return law_id\n",
        "                        else:\n",
        "                            return law_id\n",
        "    \n",
        "    return None\n",
        "\n",
        "\n",
        "# Загружаем данные один раз\n",
        "NORMALIZED_INDEX, ALL_ALIASES = load_law_aliases_with_morphology()\n",
        "\n",
        "\n",
        "def parse_legal_reference_v2(text: str) -> Dict[str, Optional[str]]:\n",
        "    \"\"\"\n",
        "    Парсит юридический текст и извлекает номера статьи, пункта и подпункта.\n",
        "    Версия с поддержкой склонений через pymorphy3.\n",
        "    \n",
        "    Args:\n",
        "        text: Текст с упоминанием статьи, пункта и/или подпункта\n",
        "        \n",
        "    Returns:\n",
        "        Словарь с полями law_id, article, point_article, subpoint_article\n",
        "    \"\"\"\n",
        "    result = {\n",
        "        \"law_id\": None,\n",
        "        \"article\": None,\n",
        "        \"point_article\": None,\n",
        "        \"subpoint_article\": None\n",
        "    }\n",
        "    \n",
        "    text = text.lower().strip()\n",
        "    \n",
        "    # Поиск law_id с учетом склонений\n",
        "    result[\"law_id\"] = find_law_in_text(text, NORMALIZED_INDEX, ALL_ALIASES)\n",
        "    \n",
        "    # Паттерны для поиска статьи\n",
        "    # Поддержка форматов: 115, 115.3, 17-2.1 и т.д. (обязательно заканчивается цифрой)\n",
        "    article_patterns = [\n",
        "        r'стать[яиеюй]?\\s+(\\d+(?:[.-]\\d+)*)',\n",
        "        r'ст\\.?\\s*(\\d+(?:[.-]\\d+)*)',\n",
        "        r'(?<!пункт\\s)(?<!пункта\\s)(?<!подпункт\\s)(?<!подпункта\\s)(\\d+(?:[.-]\\d+)*)\\s+стать[яиеюй]?',\n",
        "        r'(?<!пункт\\s)(?<!пункта\\s)(?<!подпункт\\s)(?<!подпункта\\s)(\\d+(?:[.-]\\d+)*)\\s+ст\\.?(?!\\s*\\d)',\n",
        "    ]\n",
        "    \n",
        "    # Паттерны для поиска пункта\n",
        "    point_patterns = [\n",
        "        r'(?<!под)пункт[аеуыои]?\\s+(\\d+[а-я]?)',\n",
        "        r'(?<![а-я])п\\.?\\s*(\\d+[а-я]?)',\n",
        "    ]\n",
        "    \n",
        "    # Паттерны для поиска подпункта\n",
        "    subpoint_patterns = [\n",
        "        r'подпункт[аеуыои]?\\s+([а-я\\d]+)',\n",
        "        r'подп\\.?\\s*([а-я\\d]+)',\n",
        "        r'пп\\.?\\s*([а-я\\d]+)',\n",
        "    ]\n",
        "    \n",
        "    # Поиск статьи\n",
        "    for pattern in article_patterns:\n",
        "        match = re.search(pattern, text)\n",
        "        if match:\n",
        "            result[\"article\"] = match.group(1)\n",
        "            break\n",
        "    \n",
        "    # Поиск пункта\n",
        "    for pattern in point_patterns:\n",
        "        match = re.search(pattern, text)\n",
        "        if match:\n",
        "            result[\"point_article\"] = match.group(1)\n",
        "            break\n",
        "    \n",
        "    # Поиск подпункта\n",
        "    for pattern in subpoint_patterns:\n",
        "        match = re.search(pattern, text)\n",
        "        if match:\n",
        "            result[\"subpoint_article\"] = match.group(1)\n",
        "            break\n",
        "    \n",
        "    return result\n",
        "\n",
        "\n",
        "# Тестирование функции с расширенным набором тестов\n",
        "test_cases_v2 = [\n",
        "    \"В ходе судебного заседания по вопросу наследования было установлено, что права истца на получение имущества защищены законодательством. В частности, это регулируется пп. ж п. 2 ст. 1506 Семейного кодекса РФ, что позволяет оспорить права других наследников. Суд принял решение в пользу истца, основываясь на данной норме законодательства.\",\n",
        "]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ТЕСТИРОВАНИЕ ВЕРСИИ 2 (с поддержкой склонений через pymorphy3)\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "for test in test_cases_v2:\n",
        "    result = parse_legal_reference_v2(test)\n",
        "    print(f\"Входной текст: '{test}'\")\n",
        "    print(f\"Результат: {result}\")\n",
        "    print()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
